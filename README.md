# ğŸš€ LLM Stream Parser

<div align="center">

[![npm version](https://img.shields.io/npm/v/llm-stream-parser.svg?style=for-the-badge)](https://www.npmjs.com/package/llm-stream-parser)
[![TypeScript](https://img.shields.io/badge/TypeScript-007ACC?style=for-the-badge&logo=typescript&logoColor=white)](https://www.typescriptlang.org/)
[![Node.js](https://img.shields.io/badge/Node.js-43853D?style=for-the-badge&logo=node.js&logoColor=white)](https://nodejs.org/)
[![MIT License](https://img.shields.io/badge/License-MIT-yellow.svg?style=for-the-badge)](https://opensource.org/licenses/MIT)

**ğŸ¯ Professional-grade XML parser optimized for Large Language Model streaming
responses**

_Real-time parsing â€¢ Type-safe â€¢ Event-driven â€¢ Production-ready_

</div>

---

## ğŸŒŸ Overview

**LLM Stream Parser** is a high-performance, TypeScript-first library designed
specifically for parsing structured XML tags from streaming Large Language Model
responses. Whether you're building AI chatbots, processing OpenAI streams, or
handling Claude responses, this library provides the robust, real-time parsing
capabilities you need.

### ğŸ¯ **Why LLM Stream Parser?**

- **ğŸ”„ Stream-First Design**: Parse content as it arrives, no waiting for
  completion
- **ğŸ§  LLM-Optimized**: Built specifically for AI model output patterns
- **âš¡ High Performance**: >10,000 tags/second with intelligent buffering
- **ğŸ›¡ï¸ Type-Safe**: Full TypeScript support with strict typing
- **ğŸ—ï¸ Dual Parsing Modes**: Simple flat parsing or complex nested hierarchies
- **ğŸ¨ Flexible Configuration**: Extensive customization and validation options
- **ğŸ“Š Event-Driven**: Real-time monitoring with comprehensive event system
- **ğŸ§ª Production Ready**: Comprehensive testing, error handling, and
  documentation

## ğŸ“‹ Table of Contents

- [ğŸŒŸ Overview](#-overview)
- [âš¡ Quick Start](#-quick-start)
- [ğŸ“¦ Installation](#-installation)
- [ğŸ¯ Real-World Example](#-real-world-example)
- [ğŸ“– API Reference](#-api-reference)
- [ğŸ”§ Configuration](#-configuration)
- [ğŸ¨ Advanced Usage](#-advanced-usage)
- [ğŸš€ Performance](#-performance)
- [ğŸ§ª Testing](#-testing)
- [ğŸ¤ Contributing](#-contributing)

## âœ¨ Key Features

<table>
<tr>
<td width="50%">

### ğŸ¯ **Core Capabilities**

- âœ… **Stream Processing** - Real-time parsing as data arrives
- âœ… **Dual Modes** - Flat parsing or nested hierarchies
- âœ… **Type Safety** - Full TypeScript support
- âœ… **Event System** - Real-time monitoring and callbacks
- âœ… **Error Recovery** - Robust error handling and recovery

</td>
<td width="50%">

### ğŸ”§ **Advanced Features**

- âœ… **Validation** - Content and attribute validation
- âœ… **Transformation** - Built-in content transformers
- âœ… **Buffer Management** - Intelligent memory management
- âœ… **Performance** - Optimized for high throughput
- âœ… **Extensibility** - Modular, customizable architecture

</td>
</tr>
</table>

## ğŸ“¦ Installation

```bash
npm install llm-stream-parser
```

```bash
yarn add llm-stream-parser
```

```bash
pnpm add llm-stream-parser
```

## âš¡ Quick Start

Get up and running in under 2 minutes:

### ğŸš€ **Basic Usage**

```typescript
import { LLMStreamParser } from 'llm-stream-parser';

// 1. Create parser
const parser = new LLMStreamParser();

// 2. Define tags
parser.addSimpleTags(['thinking', 'response', 'code']);

// 3. Handle events
parser.on('tag_completed', tag => {
  console.log(`âœ… ${tag.tagName}: ${tag.content}`);
});

// 4. Parse streaming content
parser.parse('<thinking>Analyzing the problem...</thinking>');
parser.parse("<response>Here's the solution:</response>");
parser.finalize();
```

### ğŸ§  **LLM Integration Example**

```typescript
// Perfect for Gemini, OpenAI, Claude, or any streaming LLM
import * as dotenv from 'dotenv';
import { GoogleGenAI } from '@google/genai';

import { LLMStreamParser } from 'llm-stream-parser';

dotenv.config();

const systemPrompt = `
You are an instructor creating technical training content.
You must respond in the following XML format:

<lesson>
  <title>Title here</title>
  
  <step number="1" difficulty="easy">
    <explanation>exp here</explanation>
    <code>code here</code>
  </step>
  
  <summary>summary_here</summary>
</lesson>
`;

const GEMINI_API_KEY = process.env.GEMINI_API_KEY;

const ai = new GoogleGenAI({ apiKey: GEMINI_API_KEY });

async function main() {
  const question = 'Create a lesson on developing a REST API with Express.js';
  const parser = new LLMStreamParser();
  parser.on('tag_completed', tag => {
    console.log(`${JSON.stringify(tag)}\n`);
  });

  parser.addSimpleTags([
    'lesson',
    'title',
    'step',
    'explanation',
    'code',
    'summary',
  ]);

  const response = await ai.models.generateContentStream({
    model: 'gemini-2.0-flash',
    contents: `${systemPrompt}\n\n${question}`,
  });

  for await (const chunk of response) {
    if (chunk.text) {
      parser.parse(chunk.text);
    }
  }
}

main().catch(console.error);
```

## ğŸ¯ Real-World Example

See the parser in action with our **live streaming demo** that simulates actual
LLM responses:

### ğŸŒŠ **Chunk Streaming Demo**

```bash
# Run the real-world streaming demo
node demo-streaming.js
```

This demo showcases:

- **Realistic chunk streaming** (25-character chunks with delays)
- **Complex nested XML** parsing in real-time
- **Event-driven processing** with live updates
- **Performance monitoring** and statistics
- **Error handling** and recovery

**Sample Output:**

```
ğŸš€ LLM Stream Parser - Real World Demo

ğŸ§ª Starting Chat Conversation Demo

ğŸ“‹ Registered 10 tag types

ğŸŒŠ Starting streaming simulation...

ğŸ“¦ Chunk 1/47: "<conversation id=\"support"
ğŸ”“ Tag opened: <conversation> (depth: 1)
ğŸ“¦ Chunk 2/47: "-chat\" timestamp=\"2024-0"
ğŸ“¦ Chunk 3/47: "1-15T14:30:00Z\">\n  <me"
ğŸ”“ Tag opened: <message> (depth: 2)
ğŸ“¦ Chunk 4/47: "ssage role=\"user\" id=\"m"
...
âœ… Tag completed: <thinking> with 108 chars
   ğŸ“ Content preview: "The user is experiencing login issues. I should ga..."
âœ… Tag completed: <message> with 891 chars
ğŸ‰ Parsing complete! Total tags: 15

ğŸ“Š DEMO RESULTS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â±ï¸  Processing time: 2485ms
ğŸ“¦ Total chunks: 47
ğŸ“ Content length: 1174 characters
ğŸ·ï¸  Tags completed: 15
ğŸš¨ Errors: 0
âœ… Success rate: 100.0%
```

### ğŸ¨ **Advanced Demo Features**

The demo includes sophisticated examples like:

- **Chat conversations** with nested thinking blocks
- **AI reasoning** with evidence and analysis
- **Code generation** with syntax highlighting
- **Real-time monitoring** with performance metrics
- **Error simulation** and recovery testing

Perfect for testing integration with OpenAI, Claude, Gemini, or any LLM that
uses XML-structured responses.

## ğŸ“– API Reference

### ğŸ—ï¸ **Core Classes**

<details>
<summary><strong>ğŸ“˜ LLMStreamParser&lt;T&gt; - Main API Class</strong></summary>

The primary interface for all parsing operations.

```typescript
class LLMStreamParser<T extends BaseTag = BaseTag> {
  constructor(config?: ParserConfig);
}
```

#### **ğŸ”§ Tag Management**

```typescript
// Define custom tags with validation and transformation
defineTag(definition: TagDefinition<T>): this;
defineTags(definitions: TagDefinition<T>[]): this;

// Quick setup for simple tags
addSimpleTag(tagName: string, options?: SimpleTagOptions): this;
addSimpleTags(tagNames: string[]): this;

// Tag registry management
removeTag(tagName: string): boolean;
hasTag(tagName: string): boolean;
getRegisteredTags(): readonly string[];
```

#### **âš¡ Parsing Operations**

```typescript
// Stream processing
parse(chunk: string): void;           // Process individual chunks
parseComplete(content: string): void; // Parse complete content at once
finalize(): void;                     // Complete parsing session
reset(): void;                        // Reset parser state
```

#### **ğŸ“Š State & Monitoring**

```typescript
getState(): ParserState;              // Current parser state
getStats(): ParserStats;              // Performance statistics
getCurrentDepth(): number;            // Current nesting depth
getCurrentPath(): string;             // Current tag path
getBufferSize(): number;              // Current buffer usage
```

#### **ğŸ¯ Event System**

```typescript
// Event listeners
on<K extends keyof ParserEvents<T>>(event: K, listener: ParserEvents<T>[K]): this;
off<K extends keyof ParserEvents<T>>(event: K, listener: ParserEvents<T>[K]): this;
once<K extends keyof ParserEvents<T>>(event: K, listener: ParserEvents<T>[K]): this;

// Utility methods
clone(): LLMStreamParser<T>;          // Create parser copy
```

</details>

### ğŸ¯ **Key Types & Interfaces**

<details>
<summary><strong>ğŸ“‹ ParserConfig - Configuration Options</strong></summary>

```typescript
interface ParserConfig {
  // ğŸ”§ Basic Configuration
  maxBufferSize?: number; // Default: 1MB - Buffer size limit
  trimWhitespace?: boolean; // Default: true - Auto-trim content
  caseSensitive?: boolean; // Default: true - Case-sensitive tags

  maxDepth?: number; // Default: 10 - Max nesting depth
  autoCloseUnclosed?: boolean; // Default: false - Auto-close tags

  // âš¡ Performance Configuration
  batchSize?: number; // Default: 1000 - Processing batch size
  enableStatistics?: boolean; // Default: true - Track performance
}
```

#### **ğŸ¯ Common Configurations**

```typescript
// ğŸš€ High Performance Setup
const performanceConfig: ParserConfig = {
  maxBufferSize: 2 * 1024 * 1024, // 2MB buffer
  batchSize: 2000, // Large batches
  enableStatistics: false, // Skip stats for speed
  trimWhitespace: false, // Skip trimming
};

// ğŸ”’ Strict Validation Setup
const strictConfig: ParserConfig = {
  caseSensitive: true, // Exact case matching
  autoCloseUnclosed: false, // Require proper closing
  enableStatistics: true, // Monitor for errors
  maxDepth: 5, // Limit nesting
};

// ğŸ§  LLM Optimized Setup
const llmConfig: ParserConfig = {
  maxDepth: 15, // Deep nesting for AI
  autoCloseUnclosed: true, // Forgive LLM errors
  trimWhitespace: true, // Clean AI output
};
```

</details>

<details>
<summary><strong>ğŸ·ï¸ TagDefinition - Advanced Tag Configuration</strong></summary>

```typescript
interface TagDefinition<T extends BaseTag = BaseTag> {
  readonly tagName: T['tagName'];

  // âœ… Validation
  validateContent?: (content: string) => ValidationResult;
  validateAttributes?: (attributes?: T['attributes']) => ValidationResult;
  validateChildren?: (children: NestedTag[]) => ValidationResult;

  // ğŸ”„ Transformation
  transformContent?: (content: string) => string;
  transformAttributes?: (
    attributes?: Record<string, unknown>
  ) => T['attributes'];

  // âš™ï¸ Configuration
  allowChildren?: boolean; // Default: true
  allowSelfClosing?: boolean; // Default: true

  // ğŸ¯ Event Handlers
  onStart?: (tagName: T['tagName'], attributes?: T['attributes']) => void;
  onContentUpdate?: (partialContent: string, tag: Partial<T>) => void;
  onComplete?: (tag: T) => void;
  onChildAdded?: (child: NestedTag, parent: T) => void;
}
```

</details>

## ğŸ”§ Configuration

### âš™ï¸ **Default Configuration**

```typescript
const defaultConfig: ParserConfig = {
  maxBufferSize: 1024 * 1024, // 1MB buffer
  trimWhitespace: true, // Auto-trim content
  caseSensitive: true, // Case-sensitive tags
  maxDepth: 10, // Max nesting depth
  autoCloseUnclosed: false, // Strict tag closing
};
```

### ğŸ¯ **Configuration Examples**

<details>
<summary><strong>ğŸš€ OpenAI/ChatGPT Integration</strong></summary>

```typescript
import { LLMStreamParser } from 'llm-stream-parser';

const chatGPTParser = new LLMStreamParser({
  maxDepth: 8, // Reasonable nesting limit
  autoCloseUnclosed: true, // Forgive incomplete tags
  trimWhitespace: true, // Clean output
  caseSensitive: false, // Flexible tag matching
});

// Define common ChatGPT tags
chatGPTParser.addSimpleTags([
  'thinking',
  'analysis',
  'answer',
  'code',
  'explanation',
  'summary',
]);

// Handle streaming response
chatGPTParser.on('tag_completed', tag => {
  console.log(`ChatGPT ${tag.tagName}:`, tag.content);
});
```

</details>

<details>
<summary><strong>ğŸ§  Claude/Anthropic Integration</strong></summary>

```typescript
const claudeParser = new LLMStreamParser({
  maxDepth: 12, // Claude can be deeply nested
  autoCloseUnclosed: true, // Handle incomplete responses
  maxBufferSize: 2 * 1024 * 1024, // Larger buffer for long responses
});

// Claude-specific tags
claudeParser.addSimpleTags([
  'thinking',
  'reasoning',
  'analysis',
  'conclusion',
  'example',
  'code',
  'explanation',
  'clarification',
]);
```

</details>

<details>
<summary><strong>âš¡ High-Performance Setup</strong></summary>

```typescript
const performanceParser = new LLMStreamParser({
  maxBufferSize: 4 * 1024 * 1024, // 4MB buffer
  batchSize: 5000, // Large batch processing
  enableStatistics: false, // Disable overhead
  trimWhitespace: false, // Skip trimming
});

// Minimal event handling for maximum speed
performanceParser.on('tag_completed', tag => {
  // Process tags with minimal overhead
  processTag(tag.tagName, tag.content);
});
```

</details>

## ğŸ¨ Advanced Usage

### ğŸ¤– **LLM Integration Patterns**

<details>
<summary><strong>ğŸ’¬ Real-Time Chat Applications</strong></summary>

```typescript
import { LLMStreamParser } from 'llm-stream-parser';

class AIChatHandler {
  private parser = new LLMStreamParser({
    autoCloseUnclosed: true,
  });

  constructor() {
    this.setupParser();
  }

  private setupParser() {
    // Define chat-specific tags
    this.parser.addSimpleTags([
      'thinking',
      'response',
      'code',
      'explanation',
      'question',
    ]);

    // Real-time UI updates
    this.parser.on('tag_started', tagName => {
      this.showTypingIndicator(tagName);
    });

    this.parser.on('tag_content_update', (tagName, content) => {
      this.updateChatMessage(tagName, content);
    });

    this.parser.on('tag_completed', tag => {
      this.finalizeChatMessage(tag);
    });
  }

  async handleStreamingResponse(stream: ReadableStream) {
    const reader = stream.getReader();

    try {
      while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        const chunk = new TextDecoder().decode(value);
        this.parser.parse(chunk);
      }
    } finally {
      this.parser.finalize();
      reader.releaseLock();
    }
  }

  private showTypingIndicator(tagName: string) {
    // Update UI to show AI is generating specific content type
    console.log(`ğŸ¤– AI is generating ${tagName}...`);
  }

  private updateChatMessage(tagName: string, content: string) {
    // Real-time content updates in chat interface
    document.getElementById(`${tagName}-content`).textContent = content;
  }

  private finalizeChatMessage(tag: any) {
    // Apply final formatting and save to chat history
    console.log(`âœ… Completed ${tag.tagName}:`, tag.content);
  }
}
```

</details>

<details>
<summary><strong>ğŸ”¬ Complex AI Reasoning Analysis</strong></summary>

```typescript
import {
  LLMStreamParser,
  ContentValidators,
  ContentTransformers,
} from 'llm-stream-parser';

class ReasoningAnalyzer {
  private parser = new LLMStreamParser({
    maxDepth: 15,
  });

  constructor() {
    this.setupAdvancedTags();
  }

  private setupAdvancedTags() {
    // Evidence tag with validation
    this.parser.defineTag({
      tagName: 'evidence',
      validateAttributes: attrs => {
        if (!attrs?.source) return 'Evidence must have a source';
        if (
          !attrs?.strength ||
          !['weak', 'medium', 'strong'].includes(attrs.strength as string)
        ) {
          return 'Evidence strength must be weak, medium, or strong';
        }
        return true;
      },
      transformContent: ContentTransformers.trim(),
      onComplete: tag => {
        this.analyzeEvidence(tag);
      },
    });

    // Analysis tag with confidence scoring
    this.parser.defineTag({
      tagName: 'analysis',
      validateAttributes: attrs => {
        const confidence = attrs?.confidence as number;
        if (confidence && (confidence < 0 || confidence > 1)) {
          return 'Confidence must be between 0 and 1';
        }
        return true;
      },
      onComplete: tag => {
        this.trackAnalysisQuality(tag);
      },
    });

    // Conclusion with summary generation
    this.parser.defineTag({
      tagName: 'conclusion',
      transformContent: ContentTransformers.chain(
        ContentTransformers.trim(),
        ContentTransformers.normalizeWhitespace()
      ),
      onComplete: tag => {
        this.generateSummary(tag);
      },
    });
  }

  private analyzeEvidence(tag: any) {
    const strength = tag.attributes?.strength;
    const source = tag.attributes?.source;

    console.log(`ğŸ“Š Evidence Analysis:
      Source: ${source}
      Strength: ${strength}
      Content: ${tag.content.substring(0, 100)}...`);
  }

  private trackAnalysisQuality(tag: any) {
    const confidence = tag.attributes?.confidence || 0.5;
    console.log(`ğŸ¯ Analysis Quality Score: ${(confidence * 100).toFixed(1)}%`);
  }

  private generateSummary(tag: any) {
    console.log(`ğŸ“ Conclusion Summary: ${tag.content}`);
  }
}
```

</details>

## ğŸ”§ Advanced Configuration

### Custom Validators

```typescript
import { ContentValidators, AttributeValidators } from 'llm-stream-parser';

const parser = createParser();

parser.defineTag({
  tagName: 'email',
  validateContent: ContentValidators.pattern(
    /^[^\s@]+@[^\s@]+\.[^\s@]+$/,
    'Invalid email format'
  ),
  validateAttributes: AttributeValidators.required(['type']),
  transformContent: ContentTransformers.toLowerCase(),
});

parser.defineTag({
  tagName: 'phone',
  validateContent: ContentValidators.custom(content => {
    const cleaned = content.replace(/\D/g, '');
    return cleaned.length >= 10 || 'Phone number must have at least 10 digits';
  }),
  transformContent: content => content.replace(/\D/g, ''),
});
```

### Custom Transformers

```typescript
import { ContentTransformers } from 'llm-stream-parser';

// Create custom transformer
const sanitizeHtml = ContentTransformers.custom(
  content => content.replace(/<script[^>]*>.*?<\/script>/gi, ''),
  'Failed to sanitize HTML'
);

parser.defineTag({
  tagName: 'content',
  transformContent: ContentTransformers.chain(
    ContentTransformers.stripHtml(),
    sanitizeHtml,
    ContentTransformers.trim(),
    ContentTransformers.normalizeWhitespace()
  ),
});
```

### Buffer Management

```typescript
const parser = createParser({
  maxBufferSize: 512 * 1024, // 512KB limit
  batchSize: 500, // Process in smaller batches
});

// Monitor buffer usage
parser.on('stats_updated', stats => {
  const usage = (stats.bufferSize / (512 * 1024)) * 100;
  if (usage > 80) {
    console.warn(`Buffer usage high: ${usage.toFixed(1)}%`);
  }
});
```

## ğŸ—ï¸ Architecture

### Modular Design

The library follows a clean modular architecture with focused responsibilities:

```
src/
â”œâ”€â”€ types/                    # Type definitions (6 modules)
â”‚   â”œâ”€â”€ base.ts              # Core types (BaseTag, NestedTag, etc.)
â”‚   â”œâ”€â”€ config.ts            # Configuration interfaces
â”‚   â”œâ”€â”€ errors.ts            # Error handling types
â”‚   â”œâ”€â”€ events.ts            # Event system types
â”‚   â”œâ”€â”€ schema.ts            # Schema definition types
â”‚   â””â”€â”€ index.ts             # Type exports
â”œâ”€â”€ core/                    # Core functionality (5 modules)
â”‚   â”œâ”€â”€ buffer-manager.ts    # Stream buffer management
â”‚   â”œâ”€â”€ tag-matcher.ts       # XML pattern matching
â”‚   â”œâ”€â”€ validator.ts         # Content validation
â”‚   â”œâ”€â”€ transformer.ts       # Content transformation
â”‚   â””â”€â”€ stream-parser.ts     # Main parsing engine
â”œâ”€â”€ llm-stream-parser.ts     # User-facing API
â””â”€â”€ index.ts                 # Main exports
```

### Design Principles

- **ğŸ”¨ Single Responsibility**: Each module has one focused purpose
- **ğŸ“¦ Modular**: Files kept under 300 lines for maintainability
- **ğŸ›¡ï¸ Type Safety**: Full TypeScript coverage with strict typing
- **âš¡ Performance**: Optimized for high-throughput streaming
- **ğŸ”„ Extensibility**: Easy to extend with new functionality
- **ğŸ§ª Testability**: Isolated modules for comprehensive testing

## ğŸš€ Performance

### ğŸ“Š **Benchmarks**

<table>
<tr>
<td width="50%">

#### **ğŸ¯ Processing Speed**

- **Throughput**: >10,000 tags/second
- **Latency**: <1ms average completion
- **Scalability**: Linear with content size
- **Memory**: O(1) with buffer limits

</td>
<td width="50%">

#### **âš¡ Real-World Performance**

- **OpenAI Streams**: 15-25ms processing
- **Large Documents**: 500MB+ supported
- **Concurrent Parsers**: 100+ instances
- **Memory Usage**: <10MB typical

</td>
</tr>
</table>

### ğŸ”§ **Optimization Strategies**

<details>
<summary><strong>âš¡ High-Throughput Configuration</strong></summary>

```typescript
// For maximum performance (>50,000 tags/sec)
const speedParser = new LLMStreamParser({
  maxBufferSize: 8 * 1024 * 1024, // 8MB buffer
  batchSize: 10000, // Large batches
  enableStatistics: false, // Disable overhead
  trimWhitespace: false, // Skip processing
  caseSensitive: false, // Faster matching
});

// Minimal event handling
speedParser.on('tag_completed', tag => {
  // Direct processing without logging
  processTagDirectly(tag.tagName, tag.content);
});
```

</details>

<details>
<summary><strong>ğŸ’¾ Memory-Optimized Configuration</strong></summary>

```typescript
// For memory-constrained environments (<1MB usage)
const memoryParser = new LLMStreamParser({
  maxBufferSize: 32 * 1024, // 32KB buffer
  batchSize: 50, // Small batches
  enableStatistics: true, // Monitor usage
  trimWhitespace: true, // Clean up immediately
});

// Monitor memory usage
memoryParser.on('stats_updated', stats => {
  if (stats.bufferSize > 24 * 1024) {
    console.warn('High memory usage detected');
  }
});
```

</details>

<details>
<summary><strong>ğŸ”„ Production Deployment</strong></summary>

```typescript
// Production-ready configuration
const productionParser = new LLMStreamParser({
  maxBufferSize: 2 * 1024 * 1024, // 2MB buffer
  batchSize: 1000, // Balanced batches
  enableStatistics: true, // Monitor performance
  autoCloseUnclosed: true, // Handle malformed input
  maxDepth: 20, // Support complex nesting
});

// Comprehensive error handling
productionParser.on('parse_error', (error, context) => {
  logger.error('Parser error', { error, context });
  metrics.increment('parser.errors');
});

// Performance monitoring
productionParser.on('stats_updated', stats => {
  metrics.gauge('parser.buffer_size', stats.bufferSize);
  metrics.gauge('parser.tags_processed', stats.totalTagsParsed);
});
```

</details>

## ğŸ§ª Testing

### Unit Tests

```typescript
import { createParser } from 'llm-stream-parser';

describe('LLMStreamParser', () => {
  let parser: LLMStreamParser;

  beforeEach(() => {
    parser = createParser();
  });

  test('should parse simple tags', () => {
    parser.addSimpleTag('test');

    const results: any[] = [];
    parser.on('tag_completed', tag => results.push(tag));

    parser.parseComplete('<test>Hello World</test>');

    expect(results).toHaveLength(1);
    expect(results[0].tagName).toBe('test');
    expect(results[0].content).toBe('Hello World');
  });

  test('should handle streaming content', () => {
    parser.addSimpleTag('stream');

    const results: any[] = [];
    parser.on('tag_completed', tag => results.push(tag));

    parser.parse('<stream>Part 1 ');
    parser.parse('Part 2 ');
    parser.parse('Part 3</stream>');
    parser.finalize();

    expect(results[0].content).toBe('Part 1 Part 2 Part 3');
  });
});
```

### Integration Tests

```typescript
describe('Integration Tests', () => {
  test('should handle complex nested structure', () => {
    const parser = createParser();
    parser.addSimpleTags(['doc', 'section', 'para']);

    const structure: any[] = [];
    parser.on('tag_closed', (tag, depth, path) => {
      structure.push({ tag: tag.tagName, depth, path });
    });

    parser.parseComplete(`
      <doc>
        <section>
          <para>Content</para>
        </section>
      </doc>
    `);

    expect(structure).toHaveLength(3);
    expect(structure[0].path).toBe('doc/section/para');
  });
});
```

## ğŸ¤ Contributing

### ğŸ› ï¸ **Development Setup**

```bash
# Clone the repository
git clone https://github.com/brankrts/llm-stream-parser.git
cd llm-stream-parser

# Install dependencies
npm install

# Run tests
npm test

# Build the library
npm run build

# Run linting
npm run lint

# Watch mode for development
npm run dev
```

### ğŸ“‹ **Code Standards**

- âœ… **TypeScript** - Strict mode enabled
- âœ… **ESLint** - Follow configuration rules
- âœ… **File Size** - Keep modules under 300 lines
- âœ… **Test Coverage** - Write comprehensive tests
- âœ… **Documentation** - Document public APIs

---

## ğŸ“„ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE)
file for details.

## ğŸ™ Acknowledgments

- ğŸ§  **Inspired by** the need for real-time LLM response parsing
- ğŸ—ï¸ **Built with** modern TypeScript best practices
- ğŸ¤– **Designed for** the era of AI-powered applications

## ğŸ“ Support & Resources

<div>

| Resource             | Link                                                           |
| -------------------- | -------------------------------------------------------------- |
| ğŸ“– **Documentation** | [Wiki](https://github.com/brankrts/llm-stream-parser)          |
| ğŸ› **Bug Reports**   | [Issues](https://github.com/brankrts/llm-stream-parser/issues) |
| ğŸ“§ **Email**         | [brankrts@gmail.com](mailto:brankrts@gmail.com)                |

</div>

---

<div>

### ğŸš€ **Ready to parse your LLM streams?**

```bash
npm install llm-stream-parser
```

**[â­ Star this repository](https://github.com/brankrts/llm-stream-parser)** if
you find it useful!

_Built with â¤ï¸ for the AI community_

</div>
